{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nabilLearns/Deeply-Supervised-Nets/blob/master/DSN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9JijckN_-DE"
      },
      "source": [
        "## Get required modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAaXxrNM_-DH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uN1xPzC_-DH"
      },
      "outputs": [],
      "source": [
        "!pip --quiet install wget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljJ_hcA0_-DI",
        "outputId": "d380d409-44f6-4bb1-b7fc-a05bce6f8685"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/nabil/Documents/Code/Data_Classification\n",
            "/home/nabil/Documents/Code/Data_Classification/DateData\n"
          ]
        }
      ],
      "source": [
        "#import os\n",
        "#os.path.join(os.getcwd()\n",
        "print(os.getcwd())\n",
        "print(os.path.join(os.getcwd(), 'DateData'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fa4C9wlE_-DJ"
      },
      "source": [
        "## Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRqVJE4C_-DK"
      },
      "outputs": [],
      "source": [
        "# Download dataset in an .arff final into current working directory\n",
        "\n",
        "import wget, requests, io, os, shutil\n",
        "from zipfile import ZipFile\n",
        "\n",
        "data_url = \"https://www.muratkoklu.com/datasets/vtdhnd06.php\"\n",
        "req = requests.get(data_url)\n",
        "zip_file = ZipFile(io.BytesIO(req.content))\n",
        "path = os.path.join(os.getcwd())\n",
        "zip_file.extractall(path)\n",
        "\n",
        "shutil.move(os.path.join(os.getcwd(),'Date_Fruit_Datasets/Date_Fruit_Datasets.arff'), os.path.join(os.getcwd(), 'Date_Fruit_Datasets.arff'))\n",
        "shutil.rmtree(os.path.join(os.getcwd(),'Date_Fruit_Datasets'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAMGYFjI_-DL",
        "outputId": "60296099-b58c-4655-ad7e-94db8e262045"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Date_Fruit_Datasets.arff']\n"
          ]
        }
      ],
      "source": [
        "# Python script that converts .arff to .csv (not mine)\n",
        "# Source: https://github.com/haloboy777/arfftocsv/blob/master/arffToCsv.py\n",
        "\n",
        "#########################################\n",
        "# Project   : ARFF to CSV converter     #\n",
        "# Created   : 10/01/17 11:08:06         #\n",
        "# Author    : haloboy777                #\n",
        "# Licence   : MIT                       #\n",
        "#########################################\n",
        "\n",
        "# Importing library\n",
        "import os\n",
        "\n",
        "# Getting all the arff files from the current directory\n",
        "files = [arff for arff in os.listdir('.') if arff.endswith(\".arff\")]\n",
        "print(files)\n",
        "\n",
        "# Function for converting arff list to csv list\n",
        "def toCsv(text):\n",
        "    data = False\n",
        "    header = \"\"\n",
        "    new_content = []\n",
        "    for line in text:\n",
        "        if not data:\n",
        "            if \"@ATTRIBUTE\" in line or \"@attribute\" in line:\n",
        "                attributes = line.split()\n",
        "                if(\"@attribute\" in line):\n",
        "                    attri_case = \"@attribute\"\n",
        "                else:\n",
        "                    attri_case = \"@ATTRIBUTE\"\n",
        "                column_name = attributes[attributes.index(attri_case) + 1]\n",
        "                header = header + column_name + \",\"\n",
        "            elif \"@DATA\" in line or \"@data\" in line:\n",
        "                data = True\n",
        "                header = header[:-1]\n",
        "                header += '\\n'\n",
        "                new_content.append(header)\n",
        "        else:\n",
        "            new_content.append(line)\n",
        "    return new_content\n",
        "\n",
        "\n",
        "# Main loop for reading and writing files\n",
        "for file in files:\n",
        "    with open(file, \"r\") as inFile:\n",
        "        content = inFile.readlines()\n",
        "        name, ext = os.path.splitext(inFile.name)\n",
        "        new = toCsv(content)\n",
        "        with open(name + \".csv\", \"w\") as outFile:\n",
        "            outFile.writelines(new)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORQZmoFH_-DN"
      },
      "source": [
        "## Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZejwJgVt_-DO",
        "outputId": "608f6f90-e97c-4d04-e93c-4d59de8029ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     AREA  PERIMETER  MAJOR_AXIS  MINOR_AXIS  ECCENTRICITY   EQDIASQ  \\\n",
            "0  422163   2378.908    837.8484    645.6693        0.6373  733.1539   \n",
            "1  338136   2085.144    723.8198    595.2073        0.5690  656.1464   \n",
            "2  526843   2647.394    940.7379    715.3638        0.6494  819.0222   \n",
            "3  416063   2351.210    827.9804    645.2988        0.6266  727.8378   \n",
            "4  347562   2160.354    763.9877    582.8359        0.6465  665.2291   \n",
            "\n",
            "   SOLIDITY  CONVEX_AREA  EXTENT  ASPECT_RATIO  ...  KurtosisRR  KurtosisRG  \\\n",
            "0    0.9947       424428  0.7831        1.2976  ...      3.2370      2.9574   \n",
            "1    0.9974       339014  0.7795        1.2161  ...      2.6228      2.6350   \n",
            "2    0.9962       528876  0.7657        1.3150  ...      3.7516      3.8611   \n",
            "3    0.9948       418255  0.7759        1.2831  ...      5.0401      8.6136   \n",
            "4    0.9908       350797  0.7569        1.3108  ...      2.7016      2.9761   \n",
            "\n",
            "   KurtosisRB    EntropyRR    EntropyRG    EntropyRB  ALLdaub4RR  ALLdaub4RG  \\\n",
            "0      4.2287 -59191263232 -50714214400 -39922372608     58.7255     54.9554   \n",
            "1      3.1704 -34233065472 -37462601728 -31477794816     50.0259     52.8168   \n",
            "2      4.7192 -93948354560 -74738221056 -60311207936     65.4772     59.2860   \n",
            "3      8.2618 -32074307584 -32060925952 -29575010304     43.3900     44.1259   \n",
            "4      4.4146 -39980974080 -35980042240 -25593278464     52.7743     50.9080   \n",
            "\n",
            "   ALLdaub4RB  Class  \n",
            "0     47.8400  BERHI  \n",
            "1     47.8315  BERHI  \n",
            "2     51.9378  BERHI  \n",
            "3     41.1882  BERHI  \n",
            "4     42.6666  BERHI  \n",
            "\n",
            "[5 rows x 35 columns]\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv(\"Date_Fruit_Datasets.csv\")\n",
        "print(data.head())\n",
        "\n",
        "# As you can see, the values need to be normalized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jawem9G_-DP"
      },
      "source": [
        "### Generate train/val/test data splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h48I0KJw_-DP",
        "outputId": "1f124d55-8a2d-469f-cc67-df53f86ec8d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1 2 3 4 5 6 7]\n",
            "Class 1:, <class 'pandas.core.frame.DataFrame'>, (52, 35)\n",
            "Class 2:, <class 'pandas.core.frame.DataFrame'>, (78, 35)\n",
            "Class 3:, <class 'pandas.core.frame.DataFrame'>, (163, 35)\n",
            "Class 4:, <class 'pandas.core.frame.DataFrame'>, (57, 35)\n",
            "Class 5:, <class 'pandas.core.frame.DataFrame'>, (132, 35)\n",
            "Class 6:, <class 'pandas.core.frame.DataFrame'>, (159, 35)\n",
            "Class 7:, <class 'pandas.core.frame.DataFrame'>, (75, 35)\n"
          ]
        }
      ],
      "source": [
        "# Generate train/val/test data splits\n",
        "\n",
        "#print(data.info())\n",
        "#print(data['Class'].unique())\n",
        "#normalized_df=(df-df.mean())/df.std()\n",
        "\n",
        "normalized = (data - data.mean()) / data.std()\n",
        "data = normalized\n",
        "#print(normalized)\n",
        "\n",
        "data['Class'] = data['Class'].astype('category').cat.codes + 1\n",
        "print(data['Class'].unique())\n",
        "train, val, test = {}, {}, {}\n",
        "for date_type in data['Class'].unique(): #['DOKOL']\n",
        "\n",
        "    num_examples = data[data['Class'] == date_type].shape[0]\n",
        "    data[data['Class'] == date_type] = data[data['Class'] == date_type].sample(frac=1, random_state=42)\n",
        "\n",
        "    train[date_type] = data[data['Class'] == date_type].reset_index().drop(columns=['index']).iloc[0:int(num_examples*0.8)]#.values\n",
        "    val[date_type] = data[data['Class'] == date_type].reset_index().drop(columns=['index']).iloc[int(num_examples*0.8):int(num_examples*0.9)]#.values\n",
        "    test[date_type] = data[data['Class'] == date_type].reset_index().drop(columns=['index']).iloc[int(num_examples*0.9):]#.values\n",
        "\n",
        "    print(f\"Class {date_type}:, {type(train[date_type])}, {train[date_type].shape}\")\n",
        "\n",
        "#print(len(train))\n",
        "train, val, test = [pd.concat(split).sample(frac=1, random_state=1).reset_index(drop=True) for split in [train, val, test]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OczBIN8L_-DR"
      },
      "source": [
        "### Get data and label tensors from each dataframe split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBPaJ5Lq_-DS"
      },
      "outputs": [],
      "source": [
        "# Get data and label tensors from each split dataframe\n",
        "\n",
        "X_train = torch.Tensor(train.values[:,:-1])\n",
        "Y_train = torch.Tensor(train.values[:,-1])\n",
        "\n",
        "X_val = torch.Tensor(val.values[:,:-1])\n",
        "Y_val = torch.Tensor(val.values[:,-1])\n",
        "\n",
        "X_test = torch.Tensor(test.values[:,:-1])\n",
        "Y_test = torch.Tensor(test.values[:,-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ariDqbC-_-DS",
        "outputId": "f10db865-9910-464f-f27a-972ba0096809"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([1., 2., 3., 4., 5., 6., 7.])\n"
          ]
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzfGfsD1_-DT"
      },
      "source": [
        "## Define Model Architecture and Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fw2ZIY5T_-DU"
      },
      "outputs": [],
      "source": [
        "# define model\n",
        "torch.manual_seed(0)\n",
        "\n",
        "class Network(nn.Module):\n",
        "    def __init__(self, n_in=34, n_hidden=5, n_classes=7):#, ks, stride):\n",
        "        super(Network, self).__init__()\n",
        "        self.mapping1 = nn.Linear(n_in, n_hidden)\n",
        "        self.mapping2 = nn.Linear(n_hidden, n_hidden)\n",
        "        self.mapping3 = nn.Linear(n_hidden, n_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.svm_1 = torch.empty(n_hidden)\n",
        "        nn.init.uniform_(self.svm_1, -0.01, 0.01)\n",
        "\n",
        "        self.svm_2 = torch.empty(n_hidden)\n",
        "        nn.init.uniform_(self.svm_2, -0.01, 0.01)\n",
        "\n",
        "        self.svm_3 = torch.empty(n_classes)\n",
        "        nn.init.uniform_(self.svm_3, -0.01, 0.01)\n",
        "\n",
        "        #self.w = nn.Parameter(torch.randn(1, 2), requires_grad=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        f_map1 = self.relu(self.mapping1(x))\n",
        "        f_map2 = self.relu(self.mapping2(f_map1))\n",
        "        f_map3 = self.relu(self.mapping3(f_map2))\n",
        "                \n",
        "        return [f_map1, f_map2, f_map3, self.svm_1, self.svm_2, self.svm_3]\n",
        "\n",
        "\n",
        "def companion_loss(f_map, svm_w, t_labels, n_labels=7):\n",
        "    \"\"\"\n",
        "    Implements L2-SVM companion objective loss described by equations (4) and (5) of the \"Deeply Supervised Nets\" paper ([Gallagher et. al, 2014])\n",
        "\n",
        "    parameters\n",
        "    ----------\n",
        "    f_map: of shape (BATCH_SIZE x NUM_FEATURES)\n",
        "    svm_w: svm weights\n",
        "    t_labels: vector of labels\n",
        "    \"\"\"\n",
        "\n",
        "    loss = 0\n",
        "    all_labels = torch.arange(n_labels) + 1\n",
        "\n",
        "    f_labels = [all_labels[all_labels != t_labels[i]] for i in range(len(t_labels))] # f_labels.shape = (# examples, 6), each row has labels != t_label\n",
        "    f_labels = torch.stack(f_labels)\n",
        "\n",
        "    #print(f_map.shape, svm_w.shape)#, (f_map@svm_w).shape)\n",
        "    svm_pred = torch.unsqueeze(f_map @ svm_w, dim=0).T # torch.Size([716, 1])\n",
        "\n",
        "    # < w^(m), (Z^(m), y) >\n",
        "    true_label_svm = svm_pred * torch.unsqueeze(t_labels,dim=0).T # torch.Size([716, 1])\n",
        "    \n",
        "    # < w^(m), (Z^(m), y_k) > for all k\n",
        "    f_labels_svm = svm_pred * f_labels # torch.Size([716, 6])\n",
        "\n",
        "    #print(svm_pred.shape, true_label_svm.shape, f_labels_svm.shape, f_labels.shape)\n",
        "\n",
        "    loss = torch.sum(torch.nn.functional.relu(1 - (true_label_svm - f_labels_svm))**2)#**2\n",
        "    \n",
        "    return loss\n",
        "\n",
        "#global_loss = lambda data: torch.sum([companion_loss(f_map) for f_map in Network(data)])\n",
        "def global_loss(model_out: list, Y, alphas: list, gamma = 0.05):\n",
        "    \"\"\"\n",
        "    Return overall combined objective function as specified in eq. (3) of [Gallagher et. al, 2014]\n",
        "    \n",
        "    parameters\n",
        "    ----------\n",
        "    model_out: [f_map1, f_map2, f_map3, svm1, svm2, svm3]\n",
        "    alphas: list of alpha values (see eq. (3), and pg. 4 of [Gallagher et. al, 2014])\n",
        "    gamma: see pg. 4 of [Gallagher et. al, 2014]\n",
        "    \"\"\"\n",
        "\n",
        "    n_layers = len(model_out) // 2\n",
        "    companion_losses = [companion_loss(model_out[m], model_out[m+n_layers], Y) for m in range(n_layers)]\n",
        "    \n",
        "    f_layer_loss = torch.norm(model_out[-1])**2 + companion_losses[-1]\n",
        "    modified_companion_losses = [alphas[m] * torch.nn.functional.relu(torch.norm(model_out[m + n_layers])**2 + companion_losses[m] - gamma) for m in range(n_layers - 1)]\n",
        "    global_loss = f_layer_loss + sum(modified_companion_losses)\n",
        "    \n",
        "    #print(companion_losses)\n",
        "    #print(\"norms: \", torch.norm(model_out[-1]), torch.norm(model_out[-2]))\n",
        "    return global_loss\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PF_sgiq9_-DV"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1pOG3Mn_-DV"
      },
      "outputs": [],
      "source": [
        "CEL = nn.CrossEntropyLoss(ignore_index=1)#BCELoss()\n",
        "\n",
        "def train(num_epochs = 300):\n",
        "    opt = torch.optim.SGD(model.parameters(), lr=0.0001, momentum=0.9, weight_decay=0.01)\n",
        "    epoch_losses = []\n",
        "    epoch_accs = []\n",
        "\n",
        "    alphas = [14, 13] #[0.1, 0.1]\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        avg_train_loss = 0\n",
        "        train_acc = 0\n",
        "\n",
        "        if epoch % (int)(num_epochs * 0.2) == 0:\n",
        "            print(f\"EPOCH {epoch}\")\n",
        "            #print(torch.nn.functional.softmax(model(X_train)[1][0:10],dim=1))\n",
        "            #print(torch.argmax(model(X_train)[2][:],dim=1))\n",
        "\n",
        "        batch_sz = 125\n",
        "        alphas = [a * 0.1 * (1 - epoch / num_epochs) for a in alphas]\n",
        "        \n",
        "        for batch_idx in range(0, X_train.shape[0], batch_sz):\n",
        "            opt.zero_grad()\n",
        "            L = global_loss(model(X_train[batch_idx:batch_idx+batch_sz]), Y_train[batch_idx:batch_idx+batch_sz], alphas)\n",
        "\n",
        "            #pred = torch.argmax(model(X_train[batch_idx:batch_idx+batch_sz])[2],dim=1)\n",
        "            #pred =  pred.to(dtype=torch.float32)\n",
        "            #L = CEL(pred, Y_train[batch_idx:batch_idx+batch_sz].long())\n",
        "\n",
        "            L.backward()\n",
        "            opt.step()\n",
        "            #opt.zero_grad()\n",
        "\n",
        "            avg_train_loss += L.item()\n",
        "\n",
        "            train_batch_pred = torch.argmax(model(X_train[batch_idx:batch_idx+batch_sz])[2],dim=1) + 1\n",
        "            train_batch_true = Y_train[batch_idx:batch_idx+batch_sz]\n",
        "            train_acc += np.array(train_batch_pred == train_batch_true).sum()\n",
        "        \n",
        "        epoch_losses.append(avg_train_loss / X_train.shape[0])\n",
        "        epoch_accs.append(train_acc / X_train.shape[0])\n",
        "\n",
        "    plt.plot(epoch_losses)\n",
        "    plt.title(\"Train Set Loss\")\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch #')\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(epoch_accs)\n",
        "    plt.title(\"Train Set Accuracy\")\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch #')\n",
        "    plt.show()\n",
        "    \n",
        "    return epoch_losses, epoch_accs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqRmxwxG_-DW",
        "outputId": "333e5910-68b4-4010-bc9e-da7498aa5aa2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH 0\n",
            "EPOCH 60\n",
            "EPOCH 120\n",
            "EPOCH 180\n",
            "EPOCH 240\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "([22.164995417248605,\n",
              "  7.610271773524791,\n",
              "  6.159279167985117,\n",
              "  6.011503677794387,\n",
              "  5.991674156828299,\n",
              "  5.983179017818174,\n",
              "  5.975292525477916,\n",
              "  5.967769175268418,\n",
              "  5.959527127569614,\n",
              "  5.950305448564071,\n",
              "  5.940032873739743,\n",
              "  5.928430567906555,\n",
              "  5.915123625174581,\n",
              "  5.899539116374608,\n",
              "  5.880415250469187,\n",
              "  5.856785523824851,\n",
              "  5.828105393734724,\n",
              "  5.793788036154635,\n",
              "  5.752885957003972,\n",
              "  5.704809178187195,\n",
              "  5.649884229265778,\n",
              "  5.59029737930724,\n",
              "  5.530457310170434,\n",
              "  5.475367753865332,\n",
              "  5.4287860806427854,\n",
              "  5.3913595529907905,\n",
              "  5.360015570784414,\n",
              "  5.333122935375022,\n",
              "  5.309624613330351,\n",
              "  5.288043826651973,\n",
              "  5.26833846449186,\n",
              "  5.250374948512243,\n",
              "  5.23241667507747,\n",
              "  5.21499109534578,\n",
              "  5.197881560085872,\n",
              "  5.180585253838054,\n",
              "  5.1630107943572146,\n",
              "  5.144891195457075,\n",
              "  5.125438626252073,\n",
              "  5.103642937857345,\n",
              "  5.077880305284895,\n",
              "  5.045833140112168,\n",
              "  5.004587098872862,\n",
              "  4.949454962874258,\n",
              "  4.873194774435885,\n",
              "  4.767404481685362,\n",
              "  4.628534711273023,\n",
              "  4.473848225684139,\n",
              "  4.350523154828801,\n",
              "  4.280095958176938,\n",
              "  4.2431143222574415,\n",
              "  4.220192978501986,\n",
              "  4.2013852423129805,\n",
              "  4.18372702998156,\n",
              "  4.166181255319265,\n",
              "  4.147476622512221,\n",
              "  4.128551461843139,\n",
              "  4.109731407804862,\n",
              "  4.090719723834672,\n",
              "  4.071401585413757,\n",
              "  4.052294640567716,\n",
              "  4.034178749809052,\n",
              "  4.0169804322652976,\n",
              "  4.000172108911269,\n",
              "  3.984770066245308,\n",
              "  3.97058207762308,\n",
              "  3.957762307960894,\n",
              "  3.9461310189529506,\n",
              "  3.935817057859964,\n",
              "  3.9265058719912056,\n",
              "  3.9181070594148264,\n",
              "  3.9101973805347634,\n",
              "  3.9024829118611426,\n",
              "  3.8947053621601126,\n",
              "  3.8871411797720627,\n",
              "  3.8799539491450985,\n",
              "  3.8730812712088643,\n",
              "  3.86633249634471,\n",
              "  3.860135126380281,\n",
              "  3.8540801575729966,\n",
              "  3.848244224846696,\n",
              "  3.842592463147041,\n",
              "  3.837304951758358,\n",
              "  3.8321421958880717,\n",
              "  3.8270928580001744,\n",
              "  3.8221883081191077,\n",
              "  3.817339252493235,\n",
              "  3.8124837182753577,\n",
              "  3.807464983210217,\n",
              "  3.8024680281484593,\n",
              "  3.7974535553149007,\n",
              "  3.7925015241740136,\n",
              "  3.7878913240059795,\n",
              "  3.7833702897226344,\n",
              "  3.7788934547807917,\n",
              "  3.77461856437129,\n",
              "  3.7705598543476126,\n",
              "  3.7665800382305124,\n",
              "  3.7628405693523046,\n",
              "  3.7592879567066384,\n",
              "  3.7559142299204566,\n",
              "  3.752991873458777,\n",
              "  3.7501983216354966,\n",
              "  3.7474303858240225,\n",
              "  3.7447415570307045,\n",
              "  3.742242056564246,\n",
              "  3.739862282182917,\n",
              "  3.737599378191559,\n",
              "  3.7354029650128755,\n",
              "  3.733393067088207,\n",
              "  3.7315194753295216,\n",
              "  3.729734026519946,\n",
              "  3.727934341856887,\n",
              "  3.726189043268811,\n",
              "  3.7245212320508903,\n",
              "  3.722908062641847,\n",
              "  3.721314201141869,\n",
              "  3.7198957304714777,\n",
              "  3.7184843777278282,\n",
              "  3.717076860992602,\n",
              "  3.7158553480436014,\n",
              "  3.714664139561147,\n",
              "  3.713525441771779,\n",
              "  3.7123519641727043,\n",
              "  3.711238541416616,\n",
              "  3.7101535051228614,\n",
              "  3.709139008761784,\n",
              "  3.7082290649414062,\n",
              "  3.707303052507965,\n",
              "  3.7061759479884997,\n",
              "  3.7052787908628666,\n",
              "  3.7043959122130325,\n",
              "  3.703250394853134,\n",
              "  3.7023645326411923,\n",
              "  3.701504584797268,\n",
              "  3.7005496744337028,\n",
              "  3.699679241500087,\n",
              "  3.6987264196299976,\n",
              "  3.6981054551108588,\n",
              "  3.6973154078648744,\n",
              "  3.69647114503317,\n",
              "  3.695718775914368,\n",
              "  3.6951122177379756,\n",
              "  3.694307551037666,\n",
              "  3.693475819166812,\n",
              "  3.692954249888159,\n",
              "  3.6922436932611733,\n",
              "  3.6915533396118847,\n",
              "  3.6909316505133773,\n",
              "  3.6902543392927285,\n",
              "  3.689667174270033,\n",
              "  3.6889260574426066,\n",
              "  3.6883982653058442,\n",
              "  3.6877223606216174,\n",
              "  3.687174322884842,\n",
              "  3.686459887627117,\n",
              "  3.6857464326826554,\n",
              "  3.6852557645829696,\n",
              "  3.684808145022259,\n",
              "  3.684108883308965,\n",
              "  3.6834292278609464,\n",
              "  3.6826843602697275,\n",
              "  3.682395636702383,\n",
              "  3.6818972965858503,\n",
              "  3.681088666010169,\n",
              "  3.680591604563111,\n",
              "  3.6801007233518463,\n",
              "  3.6794265235602523,\n",
              "  3.6788869250420086,\n",
              "  3.6783727720463077,\n",
              "  3.6779418604333975,\n",
              "  3.6773538429643855,\n",
              "  3.6766754661858414,\n",
              "  3.676395053970081,\n",
              "  3.675895392561758,\n",
              "  3.675262365927243,\n",
              "  3.6745694975613215,\n",
              "  3.674470784277889,\n",
              "  3.6739615754708232,\n",
              "  3.673341420775685,\n",
              "  3.672733285573608,\n",
              "  3.6724284933932,\n",
              "  3.6719431957053073,\n",
              "  3.6713653649697755,\n",
              "  3.67092635511686,\n",
              "  3.670501197516585,\n",
              "  3.670068581011042,\n",
              "  3.66951836953616,\n",
              "  3.6691895809919473,\n",
              "  3.6686053142867276,\n",
              "  3.6683290364356016,\n",
              "  3.667874213703518,\n",
              "  3.667328775928007,\n",
              "  3.6667351749356234,\n",
              "  3.6665641315822497,\n",
              "  3.6663081845757683,\n",
              "  3.6656754136751486,\n",
              "  3.664954926048577,\n",
              "  3.665006371183768,\n",
              "  3.6646236227877313,\n",
              "  3.663975081630259,\n",
              "  3.6633133275548837,\n",
              "  3.6634036442421003,\n",
              "  3.6627984926021298,\n",
              "  3.662260982577361,\n",
              "  3.6619988127127705,\n",
              "  3.6614648829625307,\n",
              "  3.6612097457800497,\n",
              "  3.660740090482062,\n",
              "  3.660326781885584,\n",
              "  3.6601149489759734,\n",
              "  3.6596784964620066,\n",
              "  3.6590977674089995,\n",
              "  3.65912048510333,\n",
              "  3.658356565336941,\n",
              "  3.6579121850722327,\n",
              "  3.6578541334780903,\n",
              "  3.6573161119855317,\n",
              "  3.656823099658476,\n",
              "  3.65663995156741,\n",
              "  3.656289383019815,\n",
              "  3.655886389023765,\n",
              "  3.655548010458493,\n",
              "  3.65513832475886,\n",
              "  3.654728383325332,\n",
              "  3.654367713288888,\n",
              "  3.6540079809433923,\n",
              "  3.6537707451335546,\n",
              "  3.653291158835981,\n",
              "  3.652898436818043,\n",
              "  3.652741203094994,\n",
              "  3.652312848821033,\n",
              "  3.651914117056564,\n",
              "  3.651702667747796,\n",
              "  3.6513204734418645,\n",
              "  3.650794300953103,\n",
              "  3.6506390704788974,\n",
              "  3.6502602433359157,\n",
              "  3.650041420366511,\n",
              "  3.6497001008614482,\n",
              "  3.649109270319592,\n",
              "  3.6488707132179643,\n",
              "  3.6487898586848595,\n",
              "  3.6480313945748954,\n",
              "  3.6479418877116796,\n",
              "  3.64758769626724,\n",
              "  3.647057261546897,\n",
              "  3.646851481006132,\n",
              "  3.6466581035592704,\n",
              "  3.64600129367253,\n",
              "  3.6459445207478614,\n",
              "  3.6453748734969667,\n",
              "  3.645073618968772,\n",
              "  3.645008577314835,\n",
              "  3.6445199977086244,\n",
              "  3.644171858633031,\n",
              "  3.643714947407472,\n",
              "  3.6434080667335893,\n",
              "  3.6431692112757506,\n",
              "  3.6426860873259646,\n",
              "  3.642604934436649,\n",
              "  3.6419604423991796,\n",
              "  3.6418202149801413,\n",
              "  3.641362962776056,\n",
              "  3.6410652459000743,\n",
              "  3.6410032730528763,\n",
              "  3.640253844873865,\n",
              "  3.639987114421482,\n",
              "  3.640007871489285,\n",
              "  3.6395665174089995,\n",
              "  3.639017371492013,\n",
              "  3.638789555213971,\n",
              "  3.638643019692192,\n",
              "  3.6381417386358677,\n",
              "  3.637966774029439,\n",
              "  3.6376914764915766,\n",
              "  3.637229407965804,\n",
              "  3.636905052142436,\n",
              "  3.6367045567688328,\n",
              "  3.6365877290011785,\n",
              "  3.6361970102320837,\n",
              "  3.635532272594601,\n",
              "  3.635691381699546,\n",
              "  3.6353982254113566,\n",
              "  3.6347291402976607,\n",
              "  3.6348394042286793,\n",
              "  3.6343824503808047,\n",
              "  3.6340453504850077,\n",
              "  3.6338071343619065,\n",
              "  3.6335357154547836,\n",
              "  3.6332931944777846,\n",
              "  3.6326978033481363,\n",
              "  3.632778231658083,\n",
              "  3.632419223891956,\n",
              "  3.6320643931127794,\n",
              "  3.631863173159807,\n",
              "  3.6314181961826772,\n",
              "  3.6312599821463643,\n",
              "  3.6309986647280903,\n",
              "  3.630616896645317],\n",
              " [0.07262569832402235,\n",
              "  0.07262569832402235,\n",
              "  0.07262569832402235,\n",
              "  0.07262569832402235,\n",
              "  0.07262569832402235,\n",
              "  0.07262569832402235,\n",
              "  0.07262569832402235,\n",
              "  0.14106145251396648,\n",
              "  0.2918994413407821,\n",
              "  0.2946927374301676,\n",
              "  0.2946927374301676,\n",
              "  0.2946927374301676,\n",
              "  0.2946927374301676,\n",
              "  0.2946927374301676,\n",
              "  0.2946927374301676,\n",
              "  0.2946927374301676,\n",
              "  0.2946927374301676,\n",
              "  0.2877094972067039,\n",
              "  0.2751396648044693,\n",
              "  0.2709497206703911,\n",
              "  0.26955307262569833,\n",
              "  0.2723463687150838,\n",
              "  0.2751396648044693,\n",
              "  0.2779329608938548,\n",
              "  0.2849162011173184,\n",
              "  0.2849162011173184,\n",
              "  0.28631284916201116,\n",
              "  0.2877094972067039,\n",
              "  0.2918994413407821,\n",
              "  0.29329608938547486,\n",
              "  0.29329608938547486,\n",
              "  0.2946927374301676,\n",
              "  0.2946927374301676,\n",
              "  0.2946927374301676,\n",
              "  0.2946927374301676,\n",
              "  0.2946927374301676,\n",
              "  0.2946927374301676,\n",
              "  0.2946927374301676,\n",
              "  0.29608938547486036,\n",
              "  0.29608938547486036,\n",
              "  0.29608938547486036,\n",
              "  0.29608938547486036,\n",
              "  0.29608938547486036,\n",
              "  0.2946927374301676,\n",
              "  0.2946927374301676,\n",
              "  0.29329608938547486,\n",
              "  0.28910614525139666,\n",
              "  0.2835195530726257,\n",
              "  0.2779329608938548,\n",
              "  0.276536312849162,\n",
              "  0.2751396648044693,\n",
              "  0.2751396648044693,\n",
              "  0.2779329608938548,\n",
              "  0.27932960893854747,\n",
              "  0.27932960893854747,\n",
              "  0.2779329608938548,\n",
              "  0.2779329608938548,\n",
              "  0.276536312849162,\n",
              "  0.2751396648044693,\n",
              "  0.2751396648044693,\n",
              "  0.2737430167597765,\n",
              "  0.2737430167597765,\n",
              "  0.2709497206703911,\n",
              "  0.2709497206703911,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.2709497206703911,\n",
              "  0.2709497206703911,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.2709497206703911,\n",
              "  0.2709497206703911,\n",
              "  0.2723463687150838,\n",
              "  0.2709497206703911,\n",
              "  0.2709497206703911,\n",
              "  0.2709497206703911,\n",
              "  0.2709497206703911,\n",
              "  0.2709497206703911,\n",
              "  0.2723463687150838,\n",
              "  0.2709497206703911,\n",
              "  0.2723463687150838,\n",
              "  0.2723463687150838,\n",
              "  0.2723463687150838,\n",
              "  0.2723463687150838,\n",
              "  0.2723463687150838,\n",
              "  0.2723463687150838,\n",
              "  0.2737430167597765,\n",
              "  0.2751396648044693,\n",
              "  0.2737430167597765,\n",
              "  0.2737430167597765,\n",
              "  0.2737430167597765,\n",
              "  0.2737430167597765,\n",
              "  0.2737430167597765,\n",
              "  0.2737430167597765,\n",
              "  0.2751396648044693,\n",
              "  0.2737430167597765,\n",
              "  0.2737430167597765,\n",
              "  0.2737430167597765,\n",
              "  0.2737430167597765,\n",
              "  0.2737430167597765,\n",
              "  0.2737430167597765,\n",
              "  0.2737430167597765,\n",
              "  0.2737430167597765,\n",
              "  0.2737430167597765,\n",
              "  0.276536312849162,\n",
              "  0.276536312849162,\n",
              "  0.276536312849162,\n",
              "  0.276536312849162,\n",
              "  0.276536312849162,\n",
              "  0.276536312849162,\n",
              "  0.276536312849162,\n",
              "  0.276536312849162,\n",
              "  0.276536312849162,\n",
              "  0.276536312849162,\n",
              "  0.276536312849162,\n",
              "  0.276536312849162,\n",
              "  0.276536312849162,\n",
              "  0.276536312849162,\n",
              "  0.2751396648044693,\n",
              "  0.2751396648044693,\n",
              "  0.2737430167597765,\n",
              "  0.2737430167597765,\n",
              "  0.2737430167597765,\n",
              "  0.2737430167597765,\n",
              "  0.2737430167597765,\n",
              "  0.2737430167597765,\n",
              "  0.2737430167597765,\n",
              "  0.2737430167597765,\n",
              "  0.2737430167597765,\n",
              "  0.2737430167597765,\n",
              "  0.2737430167597765,\n",
              "  0.2737430167597765,\n",
              "  0.2737430167597765,\n",
              "  0.2737430167597765,\n",
              "  0.2737430167597765,\n",
              "  0.2737430167597765,\n",
              "  0.2737430167597765,\n",
              "  0.2737430167597765,\n",
              "  0.2737430167597765,\n",
              "  0.2737430167597765,\n",
              "  0.2737430167597765,\n",
              "  0.2737430167597765,\n",
              "  0.2723463687150838,\n",
              "  0.2737430167597765,\n",
              "  0.2709497206703911,\n",
              "  0.2723463687150838,\n",
              "  0.2709497206703911,\n",
              "  0.2709497206703911,\n",
              "  0.2709497206703911,\n",
              "  0.2709497206703911,\n",
              "  0.2709497206703911,\n",
              "  0.2709497206703911,\n",
              "  0.2709497206703911,\n",
              "  0.2709497206703911,\n",
              "  0.2709497206703911,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.2681564245810056,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.2681564245810056,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.26955307262569833,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.26955307262569833,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.2681564245810056,\n",
              "  0.26955307262569833,\n",
              "  0.26955307262569833,\n",
              "  0.26536312849162014,\n",
              "  0.2681564245810056,\n",
              "  0.26955307262569833,\n",
              "  0.26536312849162014,\n",
              "  0.26536312849162014,\n",
              "  0.26675977653631283,\n",
              "  0.26955307262569833])"
            ]
          },
          "execution_count": 1437,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = Network()\n",
        "train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmUaAGpQ_-DW"
      },
      "outputs": [],
      "source": [
        "#print((torch.argmax(model(X_train)[2],dim=1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJD0rEeC_-DW"
      },
      "source": [
        "### Deprecated code for testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRzX_oVn_-DW"
      },
      "source": [
        "Examine weights at every layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYDDpYsm_-DW",
        "outputId": "026684d1-a9be-4c3c-ce41-a2440c565084"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([10, 5]) torch.Size([5])\n",
            "tensor([[2.6562, 3.7774, 0.3459, 1.2566, 0.0000],\n",
            "        [1.6194, 2.9163, 0.0000, 0.8579, 0.0000],\n",
            "        [1.1465, 0.0000, 0.0000, 0.6298, 0.4821],\n",
            "        [1.4496, 4.4364, 0.0000, 1.0822, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [1.6923, 0.5277, 0.0000, 1.1399, 0.6488],\n",
            "        [1.9688, 5.2606, 0.0000, 1.1872, 0.0000],\n",
            "        [1.4789, 4.2494, 1.1399, 0.9333, 0.0000],\n",
            "        [0.0000, 0.0000, 3.3393, 0.0000, 1.6777],\n",
            "        [1.4936, 3.9115, 0.0000, 1.0701, 0.0000]], grad_fn=<ReluBackward0>) \n",
            "\n",
            "  tensor([[1.5944, 0.0000, 0.0000, 2.2556, 0.0000],\n",
            "        [1.7080, 0.0000, 0.0000, 1.6231, 0.0000],\n",
            "        [1.6067, 1.7206, 0.0000, 0.2798, 0.0000],\n",
            "        [1.6616, 0.0000, 0.0000, 2.1532, 0.0000],\n",
            "        [1.6646, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [1.6118, 1.8190, 0.0000, 0.6776, 0.0000],\n",
            "        [1.6538, 0.0000, 0.0000, 2.6651, 0.0000],\n",
            "        [1.0944, 0.0000, 0.0000, 1.8052, 0.0000],\n",
            "        [0.0000, 3.6739, 0.0000, 0.0000, 0.0000],\n",
            "        [1.6876, 0.0000, 0.0000, 1.9652, 0.0000]], grad_fn=<ReluBackward0>) \n",
            "\n",
            " tensor([[ 2.5092,  2.2002,  3.3305,  2.4930,  2.1255,  3.3039,  2.0523],\n",
            "        [ 2.3738,  2.1171,  3.0749,  2.3419,  2.0558,  2.9915,  1.9177],\n",
            "        [-0.6906, -0.1610, -0.7297, -0.4851, -0.9122, -0.3108, -0.7055],\n",
            "        [ 2.5206,  2.2119,  3.3258,  2.5013,  2.1451,  3.2947,  2.0496],\n",
            "        [ 1.7975,  1.7309,  2.1674,  1.7290,  1.6648,  1.9054,  1.4412],\n",
            "        [-0.6986, -0.1792, -0.6833, -0.4687, -0.9687, -0.1836, -0.7169],\n",
            "        [ 2.6877,  2.3227,  3.5959,  2.6802,  2.2549,  3.6191,  2.1915],\n",
            "        [ 2.0156,  1.8409,  2.7117,  1.9948,  1.7079,  2.5881,  1.7288],\n",
            "        [-4.7700, -3.2386, -5.4994, -4.2494, -4.9475, -4.5216, -3.9157],\n",
            "        [ 2.4751,  2.1832,  3.2440,  2.4512,  2.1195,  3.1953,  2.0066]],\n",
            "       grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "f1, f2, f3, s1, s2, s3 = model(X_train[0:10])\n",
        "\n",
        "print(f1.shape, s1.shape)\n",
        "print(f\"{f1} \\n\\n  {f2} \\n\\n {f3}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFTr4j_o_-DY"
      },
      "source": [
        "Figure out how to compute accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3XIglCD_-DY",
        "outputId": "9224b219-8709-4814-b4b3-ae288d642218"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1., 2., 3.],\n",
            "        [3., 4., 4.]]) tensor([[3., 0.]])\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "predicted = torch.Tensor([[1, 2, 3], [3, 4, 4]])\n",
        "\n",
        "true = torch.Tensor([[3, 0]])\n",
        "    \n",
        "print(predicted, true)\n",
        "print(np.array(torch.argmax(predicted, dim=1) + 1 == true).sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ODIMS1G_-DY"
      },
      "source": [
        "Previous attempts at defining companion loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rk5c3bCY_-DY",
        "outputId": "e0a918b4-9ecd-46af-c066-7c04884f66a0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nfor row in f_labels:\\n    svm_pred = torch.unsqueeze(f_map @ svm_w, dim=0).T # shape = (# examples, 1)\\n    row = torch.unsqueeze(row, dim=0) # shape = (1, 6)\\n    f_label_svms = svm_pred.long() @ row\\n    loss += torch.sum(torch.nn.functional.relu(1 - (true_label_svm.T - f_label_svms))**2)\\nreturn loss\\n    #true_label_svm = torch.unsqueeze((f_map @ svm_w) * t_labels, dim=0).T # torch.Size([716, 1]) \\n'"
            ]
          },
          "execution_count": 603,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "deprecated code -- alternate ways I tried calculating loss\n",
        "i = 0\n",
        "for row in f_labels: # i.e. row = torch.Tensor([0, 2, 3, 4, 5, 6])\n",
        "    i += 1\n",
        "    if i % 10 == 0:\n",
        "        print(i)\n",
        "    for f_label in row:\n",
        "        # < w^(m), (Z^(m), y_k) >\n",
        "        f_label_svm = (f_map @ svm_w) * f_label\n",
        "\n",
        "        # max(0, 1 - (true_label_svm - f_label_svm))**2\n",
        "        loss += sum(torch.nn.functional.relu(1 - (true_label_svm - f_label_svm))**2)\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "for row in f_labels:\n",
        "    svm_pred = torch.unsqueeze(f_map @ svm_w, dim=0).T # shape = (# examples, 1)\n",
        "    row = torch.unsqueeze(row, dim=0) # shape = (1, 6)\n",
        "    f_label_svms = svm_pred.long() @ row\n",
        "    loss += torch.sum(torch.nn.functional.relu(1 - (true_label_svm.T - f_label_svms))**2)\n",
        "return loss\n",
        "    #true_label_svm = torch.unsqueeze((f_map @ svm_w) * t_labels, dim=0).T # torch.Size([716, 1]) \n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7Z3ZNLq_-DY"
      },
      "source": [
        "Examining initial loss values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfq6GxjS_-DY",
        "outputId": "e93bc295-a5b1-42aa-9a9c-76a7328a43cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(5558.4961, grad_fn=<AddBackward0>)"
            ]
          },
          "execution_count": 1237,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# test loss\n",
        "\n",
        "f1, f2, f3, svm1, svm2, svm3 = model(X_train)\n",
        "\n",
        "loss_1 = companion_loss(f1, svm1, Y_train)\n",
        "loss_2 = companion_loss(f2, svm2, Y_train)\n",
        "loss_3 = companion_loss(f3, svm3, Y_train)\n",
        "\n",
        "global_loss(model(X_train), Y_train, alphas=[0.1, 0.2, 0.3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJZWDO8O_-DZ"
      },
      "source": [
        "Get familiar with re-shaping of input needed in loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7F3ty-_Y_-DZ",
        "outputId": "cd4e995d-2895-482c-dcdb-955f03300064"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 9]) torch.Size([1, 3]) torch.Size([1, 9])\n",
            "tensor([[  0.,  -1.,   2.],\n",
            "        [  0.,  -2.,   4.],\n",
            "        [  0.,  -3.,   6.],\n",
            "        [  0.,  -4.,   8.],\n",
            "        [  0.,  -5.,  10.],\n",
            "        [  0.,  -6.,  12.],\n",
            "        [  0.,  -7.,  14.],\n",
            "        [  0.,  -8.,  16.],\n",
            "        [  0., -10.,  20.]]) torch.Size([9, 3])\n",
            "tensor([[  0.,  -1.,   2.],\n",
            "        [  0.,  -2.,   4.],\n",
            "        [  0.,  -3.,   6.],\n",
            "        [  0.,  -4.,   8.],\n",
            "        [  0.,  -5.,  10.],\n",
            "        [  0.,  -6.,  12.],\n",
            "        [  0.,  -7.,  14.],\n",
            "        [  0.,  -8.,  16.],\n",
            "        [  0., -10.,  20.]])\n",
            "tensor([   0.,    0., 1216.])\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Re-shaping is needed in companion_loss() because in class Network.forward(), svm weights are defined to be 1 dimension i.e. (#dim,) instead of (#dim,1).\n",
        "We didn't realize this before, so we did re-shaping of svm weights in the loss function.  Properly defining weights in the forward() could work, but then we'd have to\n",
        "change up the re-shaping stuff in the loss function\n",
        "\"\"\"\n",
        "\n",
        "a = torch.Tensor([1, 2, 3, 4, 5, 6, 7, 8, 10])\n",
        "a = torch.unsqueeze(a, dim=0)\n",
        "\n",
        "true = torch.ones(a.shape)\n",
        "\n",
        "f = torch.Tensor([0, -1, 2])\n",
        "f = torch.unsqueeze(f, dim=0)\n",
        "\n",
        "m = (a.T @ f)\n",
        "\n",
        "print(a.shape, f.shape, true.shape)\n",
        "print(m, m.shape)\n",
        "print(1 - (true.T - m))\n",
        "print(sum(torch.nn.functional.relu(1 - (true.T - m))**2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fUYvGiK_-DZ"
      },
      "source": [
        "Broadcasting multiplication of the vector of predicted svm values and the 6 vectors of false labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVSHmQSs_-DZ",
        "outputId": "9d7040af-a75e-40b2-f743-ed06638b4ce9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([9, 1]) torch.Size([9, 3])\n",
            "tensor([[ 1.],\n",
            "        [ 2.],\n",
            "        [ 3.],\n",
            "        [ 4.],\n",
            "        [ 5.],\n",
            "        [ 6.],\n",
            "        [ 7.],\n",
            "        [ 8.],\n",
            "        [10.]])\n",
            "tensor([[ 1.,  1.,  1.],\n",
            "        [ 2.,  2.,  2.],\n",
            "        [ 3.,  3.,  3.],\n",
            "        [ 4.,  4.,  4.],\n",
            "        [ 5.,  5.,  5.],\n",
            "        [ 6.,  6.,  6.],\n",
            "        [ 7.,  7.,  7.],\n",
            "        [ 8.,  8.,  8.],\n",
            "        [10., 10., 10.]])\n",
            "tensor([[  1.,   1.,   1.],\n",
            "        [  4.,   4.,   4.],\n",
            "        [  9.,   9.,   9.],\n",
            "        [ 16.,  16.,  16.],\n",
            "        [ 25.,  25.,  25.],\n",
            "        [ 36.,  36.,  36.],\n",
            "        [ 49.,  49.,  49.],\n",
            "        [ 64.,  64.,  64.],\n",
            "        [100., 100., 100.]])\n"
          ]
        }
      ],
      "source": [
        "j = torch.cat([a] * 3, dim=0) #.squeeze(dim=1)\n",
        "\n",
        "print(a.T.shape, j.T.shape)\n",
        "\n",
        "print(a.T)\n",
        "print(j.T)\n",
        "print(a.T * j.T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwUjBMCo_-Da",
        "outputId": "4ecc9044-d885-431d-ffdd-7b0c6ca9d146"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 3])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[-3.5995e+09, -7.1990e+09],\n",
              "        [-4.6115e+09, -9.2229e+09],\n",
              "        [-1.0095e+10, -2.0191e+10],\n",
              "        ...,\n",
              "        [-3.0495e+09, -6.0990e+09],\n",
              "        [-3.0662e+09, -6.1324e+09],\n",
              "        [-4.7045e+09, -9.4091e+09]], grad_fn=<MmBackward0>)"
            ]
          },
          "execution_count": 460,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "svm_w = torch.randn(f1.shape[1])\n",
        "\n",
        "g = f1 @ svm_w\n",
        "g = torch.unsqueeze(g, dim=0).T\n",
        "\n",
        "print(torch.unsqueeze(torch.Tensor([1, 2, 3]), dim=0).shape)\n",
        "\n",
        "g @ torch.unsqueeze(torch.Tensor([1, 2]), dim=0)"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "a209b2a29af6db15caeb87b5a469aa1eed57ecd42970a9baa7e0ed0c9978c8fe"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "DSN.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}